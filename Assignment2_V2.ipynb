{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "bRoMqXJApJ_c"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A002fkGLpJ_f",
        "outputId": "04024eee-6b45-42ff-a7ff-5ed109a99f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIcntqHoqtGu"
      },
      "source": [
        "Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q66uIxZDpJ_g",
        "outputId": "b13e9271-0543-4690-978c-5724171ea9c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-04 04:17:00--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip.1’\n",
            "\n",
            "fra-eng.zip.1       100%[===================>]   7.57M  5.72MB/s    in 1.3s    \n",
            "\n",
            "2025-04-04 04:17:02 (5.72 MB/s) - ‘fra-eng.zip.1’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip -o fra-eng.zip\n",
        "!mkdir data\n",
        "!mv fra.txt data/eng-fra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "-3_351aopJ_g"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkxYViM1pJ_h"
      },
      "source": [
        "Loading data files\n",
        "==================\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "`This question on Open Data Stack\n",
        "Exchange <http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages>`__\n",
        "pointed me to the open translation site http://tatoeba.org/ which has\n",
        "downloads available at http://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: http://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repo, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW7aVlucpJ_i"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "2_mIcgqbpJ_i"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbx1QgHapJ_i"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "QHKmTjvtpJ_j"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArIzOagHpJ_j"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "oorAMClxpJ_j"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 15\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am\", \"i m\",\n",
        "    \"he is\", \"he s\",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re\",\n",
        "    \"we are\", \"we re\",\n",
        "    \"they are\", \"they re\"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT03nCkNpJ_k"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVXr6OUTpJ_k",
        "outputId": "73cee6bc-6f8e-4513-ec69-86ae946a85f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 232736 sentence pairs\n",
            "Trimmed to 22907 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 7019\n",
            "eng 4638\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "#print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "c_3eS7fApJ_k"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "n5UyquLbpJ_k"
      },
      "outputs": [],
      "source": [
        "X = [i[0] for i in pairs]\n",
        "y = [i[1] for i in pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ECQXlB8qpJ_k"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "hxRKfb-VpJ_k"
      },
      "outputs": [],
      "source": [
        "train_pairs = list(zip(X_train,y_train))\n",
        "test_pairs = list(zip(X_test,y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wibBZ_rpJ_k"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
        "seq2seq network, or `Encoder Decoder\n",
        "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mo0LM7VpJ_k"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "ODrqsTDmpJ_k"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bJ6WUwVpJ_l"
      },
      "source": [
        "The Decoder\n",
        "-----------\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttKBiue4pJ_l"
      },
      "source": [
        "Simple Decoder\n",
        "^^^^^^^^^^^^^^\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "ymSMO9DJpJ_l"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Simple decoder implementation:\n",
        "        # - input: current token (initially the SOS token)\n",
        "        # - hidden: current hidden state (initially the encoder's context vector)\n",
        "        # 1. Embed the input token\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        # 2. Apply non-linearity\n",
        "        output = F.relu(output)\n",
        "        # 3. Run GRU for one step, using the current hidden state\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        # 4. Transform GRU output to vocabulary size and apply log softmax\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc38VSiRpJ_l"
      },
      "source": [
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "cge_gASYpJ_l"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PfyWcPUpJ_l"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "MnW6TYfrpJ_l"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAIgGB_NpJ_l"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Bjufs09YpJ_l"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUHDjXnipJ_l"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "V-XIs2_YpJ_l"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, epochs, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    iter = 1\n",
        "    n_iters = len(train_pairs) * epochs\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch: %d/%d\" % (epoch, epochs))\n",
        "        for training_pair in train_pairs:\n",
        "            training_pair = tensorsFromPair(training_pair)\n",
        "\n",
        "            input_tensor = training_pair[0]\n",
        "            target_tensor = training_pair[1]\n",
        "\n",
        "            loss = train(input_tensor, target_tensor, encoder,\n",
        "                        decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "\n",
        "            if iter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                            iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "            iter +=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NzUOAnWpJ_m"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "8h8NUnCjpJ_m"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms25lqq4pJ_m"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ZhVN-cvGpJ_m"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "aSrQc_8kpJ_m"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "rouge = ROUGEScore()\n",
        "\n",
        "def test(encoder, decoder, testing_pairs):\n",
        "    input = []\n",
        "    gt = []\n",
        "    predict = []\n",
        "    metric_score = {\n",
        "        \"rouge1_fmeasure\":[],\n",
        "        \"rouge1_precision\":[],\n",
        "        \"rouge1_recall\":[],\n",
        "        \"rouge2_fmeasure\":[],\n",
        "        \"rouge2_precision\":[],\n",
        "        \"rouge2_recall\":[]\n",
        "    }\n",
        "    from tqdm import tqdm\n",
        "    for i in tqdm(range(len(testing_pairs))):\n",
        "        pair = testing_pairs[i]\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        input.append(pair[0])\n",
        "        gt.append(pair[1])\n",
        "        predict.append(output_sentence)\n",
        "\n",
        "        try:\n",
        "            rs = rouge(output_sentence, pair[1])\n",
        "        except:\n",
        "            continue\n",
        "        metric_score[\"rouge1_fmeasure\"].append(rs['rouge1_fmeasure'])\n",
        "        metric_score[\"rouge1_precision\"].append(rs['rouge1_precision'])\n",
        "        metric_score[\"rouge1_recall\"].append(rs['rouge1_recall'])\n",
        "        metric_score[\"rouge2_fmeasure\"].append(rs['rouge2_fmeasure'])\n",
        "        metric_score[\"rouge2_precision\"].append(rs['rouge2_precision'])\n",
        "        metric_score[\"rouge2_recall\"].append(rs['rouge2_recall'])\n",
        "\n",
        "    metric_score[\"rouge1_fmeasure\"] = np.array(metric_score[\"rouge1_fmeasure\"]).mean()\n",
        "    metric_score[\"rouge1_precision\"] = np.array(metric_score[\"rouge1_precision\"]).mean()\n",
        "    metric_score[\"rouge1_recall\"] = np.array(metric_score[\"rouge1_recall\"]).mean()\n",
        "    metric_score[\"rouge2_fmeasure\"] = np.array(metric_score[\"rouge2_fmeasure\"]).mean()\n",
        "    metric_score[\"rouge2_precision\"] = np.array(metric_score[\"rouge2_precision\"]).mean()\n",
        "    metric_score[\"rouge2_recall\"] = np.array(metric_score[\"rouge2_recall\"]).mean()\n",
        "\n",
        "    print(\"=== Evaluation score - Rouge score ===\")\n",
        "    print(\"Rouge1 fmeasure:\\t\",metric_score[\"rouge1_fmeasure\"])\n",
        "    print(\"Rouge1 precision:\\t\",metric_score[\"rouge1_precision\"])\n",
        "    print(\"Rouge1 recall:  \\t\",metric_score[\"rouge1_recall\"])\n",
        "    print(\"Rouge2 fmeasure:\\t\",metric_score[\"rouge2_fmeasure\"])\n",
        "    print(\"Rouge2 precision:\\t\",metric_score[\"rouge2_precision\"])\n",
        "    print(\"Rouge2 recall:  \\t\",metric_score[\"rouge2_recall\"])\n",
        "    print(\"=====================================\")\n",
        "    return input,gt,predict,metric_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqgG_4-LpJ_m"
      },
      "source": [
        "Task 1: Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ihH3oUSpJ_m",
        "outputId": "89b2c12d-82af-43b9-d599-04684ed7e930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0/5\n",
            "8m 6s (- 158m 58s) (5000 4%) 3.3341\n",
            "16m 49s (- 156m 31s) (10000 9%) 2.8450\n",
            "25m 22s (- 149m 2s) (15000 14%) 2.5722\n",
            "34m 2s (- 141m 25s) (20000 19%) 2.3923\n",
            "Epoch: 1/5\n",
            "42m 33s (- 132m 55s) (25000 24%) 2.2279\n",
            "50m 58s (- 124m 11s) (30000 29%) 2.0778\n",
            "59m 26s (- 115m 37s) (35000 33%) 1.9412\n",
            "67m 59s (- 107m 12s) (40000 38%) 1.8603\n",
            "Epoch: 2/5\n",
            "76m 41s (- 98m 59s) (45000 43%) 1.7719\n",
            "85m 40s (- 90m 56s) (50000 48%) 1.6602\n",
            "94m 30s (- 82m 37s) (55000 53%) 1.5854\n",
            "103m 29s (- 74m 18s) (60000 58%) 1.5194\n",
            "Epoch: 3/5\n",
            "112m 29s (- 65m 53s) (65000 63%) 1.4550\n",
            "121m 41s (- 57m 30s) (70000 67%) 1.3896\n",
            "130m 49s (- 48m 58s) (75000 72%) 1.3315\n",
            "139m 48s (- 40m 20s) (80000 77%) 1.2722\n",
            "Epoch: 4/5\n",
            "148m 59s (- 31m 41s) (85000 82%) 1.2223\n",
            "158m 6s (- 22m 58s) (90000 87%) 1.1786\n",
            "167m 8s (- 14m 12s) (95000 92%) 1.1302\n",
            "176m 12s (- 5m 25s) (100000 97%) 1.0757\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = Decoder(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, decoder1, 5, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_LmlmCPpJ_m",
        "outputId": "f7c221f7-df66-4012-efa4-9681503754d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> je suis vraiment malheureux a ce sujet .\n",
            "= i m really unhappy about this .\n",
            "< i m really sad about this . <EOS>\n",
            "\n",
            "> je suis un bon cuisinier .\n",
            "= i m a good cook .\n",
            "< i m a good cook . <EOS>\n",
            "\n",
            "> je ne suis pas extraverti .\n",
            "= i m not outgoing .\n",
            "< i m not finicky . <EOS>\n",
            "\n",
            "> elle porte une bague de prix .\n",
            "= she is wearing a valuable ring .\n",
            "< she is wearing a strong . . <EOS>\n",
            "\n",
            "> vous conduisez horriblement mal .\n",
            "= you re a horrible driver .\n",
            "< you re embarrassing trouble . <EOS>\n",
            "\n",
            "> vous etes en securite ici .\n",
            "= you re safe here .\n",
            "< you re safe here . <EOS>\n",
            "\n",
            "> on va faire tout ce qu on peut .\n",
            "= we re going to do everything we can .\n",
            "< we re going to do everything we can . <EOS>\n",
            "\n",
            "> j en ai termine .\n",
            "= i m finished .\n",
            "< i m done . <EOS>\n",
            "\n",
            "> je suis amoureux d elle .\n",
            "= i m in love with her .\n",
            "< i am in love with her . <EOS>\n",
            "\n",
            "> tu es fatiguee .\n",
            "= you re tired .\n",
            "< you re tired . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iR7Ao_rpJ_w",
        "outputId": "48d502ec-c0f8-4383-dd07-fd31f90b8303"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20616/20616 [7:20:25<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.7188376\n",
            "Rouge1 precision:\t 0.67666286\n",
            "Rouge1 recall:  \t 0.7743015\n",
            "Rouge2 fmeasure:\t 0.5675114\n",
            "Rouge2 precision:\t 0.52466303\n",
            "Rouge2 recall:  \t 0.62704736\n",
            "=====================================\n"
          ]
        }
      ],
      "source": [
        "input,gt,predict,score = test(encoder1, decoder1, train_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQEyIZXjpJ_w",
        "outputId": "a3ed2a94-e942-4674-f70b-9587207115bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2291/2291 [1:37:40<00:00,  2.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.62125415\n",
            "Rouge1 precision:\t 0.58754724\n",
            "Rouge1 recall:  \t 0.66836804\n",
            "Rouge2 fmeasure:\t 0.43808696\n",
            "Rouge2 precision:\t 0.40724462\n",
            "Rouge2 recall:  \t 0.4832314\n",
            "=====================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input,gt,predict,score = test(encoder1, decoder1, test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFOMfMhTawpL"
      },
      "source": [
        "Task 2: Replace GRU with LSTM (Encoder and Decoder)\n",
        "=======================\n",
        "Change the GRU in Encoder and Decoder in the original code base with LSTM, run the code, and record the Rouge scores for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Q3vQ17ZHayK3"
      },
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
        "                torch.zeros(1, 1, self.hidden_size, device=device))\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
        "                torch.zeros(1, 1, self.hidden_size, device=device))\n",
        "\n",
        "# Initialize the models\n",
        "hidden_size = 256\n",
        "encoder_lstm = EncoderLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder_lstm = DecoderLSTM(hidden_size, output_lang.n_words).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-cooaeya1fS"
      },
      "source": [
        "Train the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzTWgPMMa7uB",
        "outputId": "797c873f-2b9c-46bf-cd1e-3147283f5848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0/5\n",
            "1m 22s (- 27m 7s) (5000 4%) 3.4136\n",
            "2m 48s (- 26m 6s) (10000 9%) 3.0247\n",
            "4m 10s (- 24m 33s) (15000 14%) 2.8142\n",
            "5m 33s (- 23m 5s) (20000 19%) 2.6374\n",
            "Epoch: 1/5\n",
            "6m 57s (- 21m 45s) (25000 24%) 2.4688\n",
            "8m 22s (- 20m 25s) (30000 29%) 2.3147\n",
            "9m 49s (- 19m 6s) (35000 33%) 2.1873\n",
            "11m 14s (- 17m 43s) (40000 38%) 2.1031\n",
            "Epoch: 2/5\n",
            "12m 40s (- 16m 21s) (45000 43%) 2.0131\n",
            "14m 5s (- 14m 57s) (50000 48%) 1.8935\n",
            "15m 31s (- 13m 34s) (55000 53%) 1.8073\n",
            "16m 56s (- 12m 9s) (60000 58%) 1.7653\n",
            "Epoch: 3/5\n",
            "18m 20s (- 10m 44s) (65000 63%) 1.6814\n",
            "19m 44s (- 9m 19s) (70000 67%) 1.6167\n",
            "21m 8s (- 7m 54s) (75000 72%) 1.5558\n",
            "22m 32s (- 6m 30s) (80000 77%) 1.5093\n",
            "Epoch: 4/5\n",
            "23m 57s (- 5m 5s) (85000 82%) 1.4483\n",
            "25m 21s (- 3m 41s) (90000 87%) 1.3916\n",
            "26m 45s (- 2m 16s) (95000 92%) 1.3358\n",
            "28m 9s (- 0m 52s) (100000 97%) 1.2986\n"
          ]
        }
      ],
      "source": [
        "# Train the models\n",
        "trainIters(encoder_lstm, decoder_lstm, 5, print_every=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p9Z-Zq1rJEP"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xl1vAiSKrLCz"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(encoder_lstm.state_dict(), 'encoder_lstm.pth')\n",
        "torch.save(decoder_lstm.state_dict(), 'decoder_lstm.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LopTa8Jza_UL"
      },
      "source": [
        "Evaluate randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhJ0Kr1lbBxX",
        "outputId": "7fa4df29-eeec-42e0-a829-e01a95baf73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> je suis un electricien .\n",
            "= i m an electrician .\n",
            "< i m an electrician . <EOS>\n",
            "\n",
            "> il est connu en tant que chanteur de rock .\n",
            "= he is known as a rock singer .\n",
            "< he is known as a lot of her . <EOS>\n",
            "\n",
            "> il crie beaucoup .\n",
            "= he shouts a lot .\n",
            "< he s very much . <EOS>\n",
            "\n",
            "> je ne suis pas un menteur .\n",
            "= i m no liar .\n",
            "< i m not a liar . <EOS>\n",
            "\n",
            "> ils refuserent de songer a partir .\n",
            "= they refused to think of leaving .\n",
            "< they re to to about her . <EOS>\n",
            "\n",
            "> nous nous deshabillons .\n",
            "= we re undressing .\n",
            "< we re leaving . <EOS>\n",
            "\n",
            "> ce sont tous les deux de bons professeurs .\n",
            "= they are both good teachers .\n",
            "< they re both good love they re . <EOS>\n",
            "\n",
            "> je suis desole si je vous ai derangee .\n",
            "= i m sorry if i disturbed you .\n",
            "< i m sorry if i embarrassed you . <EOS>\n",
            "\n",
            "> je suis dans la meme galere .\n",
            "= i m in the same boat .\n",
            "< i m in the same boat . <EOS>\n",
            "\n",
            "> vous etes tres flexibles .\n",
            "= you re very flexible .\n",
            "< you re very sophisticated . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate randomly\n",
        "evaluateRandomly(encoder_lstm, decoder_lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6DOSPRxbCu4"
      },
      "source": [
        "Test with test pairs and record Rouge scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FesiKN2HbKfX",
        "outputId": "36e55b64-fb4c-40ed-9251-5c1c219f3ebd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2291/2291 [03:48<00:00, 10.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.61341244\n",
            "Rouge1 precision:\t 0.578206\n",
            "Rouge1 recall:  \t 0.66123617\n",
            "Rouge2 fmeasure:\t 0.43128422\n",
            "Rouge2 precision:\t 0.3999701\n",
            "Rouge2 recall:  \t 0.4761495\n",
            "=====================================\n"
          ]
        }
      ],
      "source": [
        "# Test with test pairs and record Rouge scores\n",
        "input_lstm, gt_lstm, predict_lstm, score_lstm = test(encoder_lstm, decoder_lstm, test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO937_2YrbNP"
      },
      "source": [
        "Task 3: Replace Encoder GRU with Bi-LSTM (Decoder remains GRU)\n",
        "=======================\n",
        "Change the GRU in Encoder (not Decoder) in the original code base with bi-LSTM, run the code, and record the Rouge scores for test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVXTyY9j2XED"
      },
      "source": [
        "Change the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "aV3q8nVQ2Z-s"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # Adjust encoder outputs size based on encoder type\n",
        "    output_size = encoder.hidden_size\n",
        "    if isinstance(encoder, EncoderBiLSTM):\n",
        "        output_size = encoder.hidden_size * 2  # Double size for bidirectional\n",
        "\n",
        "    # FIX: Use square brackets instead of parentheses for shape arguments\n",
        "    encoder_outputs = torch.zeros([max_length, output_size], device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "        # Handle different encoder types\n",
        "        if isinstance(encoder, EncoderBiLSTM):\n",
        "            # For BiLSTM - output[0] already contains concatenated directions\n",
        "            encoder_outputs[ei] = encoder_output[0]\n",
        "        else:\n",
        "            # For regular encoders\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    # If using BiLSTM, we need to prepare hidden state for the decoder\n",
        "    if isinstance(encoder, EncoderBiLSTM):\n",
        "        # Take only forward direction (or average both directions)\n",
        "        h = encoder_hidden[0][0].unsqueeze(0)  # Take first direction's hidden state\n",
        "        decoder_hidden = h\n",
        "    else:\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci3nksiU9ApQ"
      },
      "source": [
        "Change the evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "DuHt6zAP9CKd"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # ...\n",
        "        # Adjust encoder outputs size based on encoder type\n",
        "        output_size = encoder.hidden_size\n",
        "        if isinstance(encoder, EncoderBiLSTM):\n",
        "            output_size = encoder.hidden_size * 2  # Double size for bidirectional\n",
        "\n",
        "        # FIX: Pass dimensions as a tuple rather than separate arguments\n",
        "        encoder_outputs = torch.zeros((max_length, output_size), device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "            # Handle different encoder types\n",
        "            if isinstance(encoder, EncoderBiLSTM):\n",
        "                encoder_outputs[ei] = encoder_output[0]\n",
        "            else:\n",
        "                encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        # If using BiLSTM, prepare hidden state for the decoder\n",
        "        if isinstance(encoder, EncoderBiLSTM):\n",
        "            h = encoder_hidden[0][0].unsqueeze(0)  # Take first direction's hidden state\n",
        "            decoder_hidden = h\n",
        "        else:\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loNRZktx2c7n"
      },
      "source": [
        "Create the encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "RKC3QTZMrfiA"
      },
      "outputs": [],
      "source": [
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderBiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # Bidirectional LSTM - setting num_layers explicitly to 1\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=1, bidirectional=True)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        # Pass the embedded input and hidden state through the BiLSTM\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        # Return output and hidden state without modifications\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # For bidirectional LSTM with 1 layer, first dimension is 2\n",
        "        h0 = torch.zeros(2, 1, self.hidden_size, device=device)\n",
        "        c0 = torch.zeros(2, 1, self.hidden_size, device=device)\n",
        "        return (h0, c0)\n",
        "\n",
        "# Initialize the models\n",
        "hidden_size = 256\n",
        "encoder_bilstm = EncoderBiLSTM(input_lang.n_words, hidden_size).to(device)\n",
        "decoder_gru = Decoder(hidden_size, output_lang.n_words).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ3qOnWarqMm"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsl12Yfbrr-N",
        "outputId": "92c247b9-99fe-40dd-a202-52f0417ed8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0/5\n",
            "1m 38s (- 32m 5s) (5000 4%) 3.4016\n",
            "3m 21s (- 31m 18s) (10000 9%) 2.9345\n",
            "5m 3s (- 29m 44s) (15000 14%) 2.7165\n",
            "6m 48s (- 28m 14s) (20000 19%) 2.5522\n",
            "Epoch: 1/5\n",
            "8m 30s (- 26m 35s) (25000 24%) 2.3439\n",
            "10m 13s (- 24m 54s) (30000 29%) 2.2036\n",
            "11m 57s (- 23m 15s) (35000 33%) 2.0774\n",
            "13m 40s (- 21m 33s) (40000 38%) 1.9698\n",
            "Epoch: 2/5\n",
            "15m 23s (- 19m 51s) (45000 43%) 1.8774\n",
            "17m 5s (- 18m 8s) (50000 48%) 1.7952\n",
            "18m 48s (- 16m 26s) (55000 53%) 1.6908\n",
            "20m 32s (- 14m 44s) (60000 58%) 1.6337\n",
            "Epoch: 3/5\n",
            "22m 15s (- 13m 2s) (65000 63%) 1.5699\n",
            "23m 57s (- 11m 19s) (70000 67%) 1.5030\n",
            "25m 40s (- 9m 36s) (75000 72%) 1.4556\n",
            "27m 23s (- 7m 54s) (80000 77%) 1.3915\n",
            "Epoch: 4/5\n",
            "29m 6s (- 6m 11s) (85000 82%) 1.3258\n",
            "30m 50s (- 4m 28s) (90000 87%) 1.2931\n",
            "32m 33s (- 2m 46s) (95000 92%) 1.2397\n",
            "34m 16s (- 1m 3s) (100000 97%) 1.1826\n"
          ]
        }
      ],
      "source": [
        "# Train the models\n",
        "trainIters(encoder_bilstm, decoder_gru, 5, print_every=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU8a7sUurtrc"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "B3KUL2wErvk5"
      },
      "outputs": [],
      "source": [
        "# Save the models\n",
        "torch.save(encoder_bilstm.state_dict(), 'encoder_bilstm.pth')\n",
        "torch.save(decoder_gru.state_dict(), 'decoder_gru.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx5u6c56B7aR"
      },
      "source": [
        "Load t he models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me-I1XcsB8sW",
        "outputId": "f20457be-32c0-4fcf-bed3-308ca320fd40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the models based on the device type\n",
        "if device.type == 'cuda':\n",
        "    # Load directly to GPU\n",
        "    encoder_bilstm.load_state_dict(torch.load('encoder_bilstm.pth'))\n",
        "    decoder_gru.load_state_dict(torch.load('decoder_gru.pth'))\n",
        "else:\n",
        "    # Load to CPU with map_location parameter\n",
        "    encoder_bilstm.load_state_dict(torch.load('encoder_bilstm.pth', map_location=device))\n",
        "    decoder_gru.load_state_dict(torch.load('decoder_gru.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDAnjmCFQuzG"
      },
      "source": [
        "Rewrite the evaluate function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "h-ZBY7eJQymP"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        # Adjust encoder outputs size based on encoder type\n",
        "        output_size = encoder.hidden_size\n",
        "        if isinstance(encoder, EncoderBiLSTM):\n",
        "            output_size = encoder.hidden_size * 2  # Double size for bidirectional\n",
        "\n",
        "        # FIX: Use square brackets for tensor dimensions\n",
        "        encoder_outputs = torch.zeros([max_length, output_size], device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "            # Handle different encoder types\n",
        "            if isinstance(encoder, EncoderBiLSTM):\n",
        "                encoder_outputs[ei] = encoder_output[0]\n",
        "            else:\n",
        "                encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        # If using BiLSTM, prepare hidden state for the decoder\n",
        "        if isinstance(encoder, EncoderBiLSTM):\n",
        "            h = encoder_hidden[0][0].unsqueeze(0)  # Take first direction's hidden state\n",
        "            decoder_hidden = h\n",
        "        else:\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mqXULNCrxg6"
      },
      "source": [
        "Evaluate randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkzlL3H4rz3V",
        "outputId": "f97e5d3b-6b92-470e-d424-08cff4bb90ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> tu te leurres .\n",
            "= you are deceiving yourself .\n",
            "< you are looking yourself . <EOS>\n",
            "\n",
            "> tu es une bonne etudiante .\n",
            "= you re a good student .\n",
            "< you re a good student . <EOS>\n",
            "\n",
            "> elle m a prete son velo .\n",
            "= she s lent me her bicycle .\n",
            "< she s her me her new . <EOS>\n",
            "\n",
            "> je me suis assure que personne ne regardait .\n",
            "= i made sure that no one was watching .\n",
            "< i m sure that he can t <EOS>\n",
            "\n",
            "> c est mon dernier avertissement !\n",
            "= i m warning you for the last time !\n",
            "< i m the last one . . <EOS>\n",
            "\n",
            "> il est fort malade .\n",
            "= he s very sick .\n",
            "< he s very ill . <EOS>\n",
            "\n",
            "> je fais mes devoirs .\n",
            "= i m doing my homework .\n",
            "< i m doing my homework . <EOS>\n",
            "\n",
            "> il n a pas encore ecrit la lettre .\n",
            "= he still has not written the letter .\n",
            "< he still not always about it . <EOS>\n",
            "\n",
            "> il est tres beau .\n",
            "= he is very handsome .\n",
            "< he s very poor . <EOS>\n",
            "\n",
            "> je l ai fait partir .\n",
            "= i made him go .\n",
            "< i made it . . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate randomly\n",
        "evaluateRandomly(encoder_bilstm, decoder_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN2helxVr1f1"
      },
      "source": [
        "Test with test pairs and record Rouge sccores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7p05yHEr6Y1",
        "outputId": "662587bb-2025-4340-f263-d4ca6aac1f6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2291/2291 [04:03<00:00,  9.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.61796856\n",
            "Rouge1 precision:\t 0.58273584\n",
            "Rouge1 recall:  \t 0.6665224\n",
            "Rouge2 fmeasure:\t 0.4351948\n",
            "Rouge2 precision:\t 0.40390694\n",
            "Rouge2 recall:  \t 0.48045906\n",
            "=====================================\n"
          ]
        }
      ],
      "source": [
        "# Test with test pairs and record Rouge scores\n",
        "input_bilstm, gt_bilstm, predict_bilstm, score_bilstm = test(encoder_bilstm, decoder_gru, test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_fq5rIqtV7d"
      },
      "source": [
        "Task 4: Add Attention Mechanism between Encoder and Decoder\n",
        "=======================\n",
        "Add the attention mechanism between Encoder and Decoder in the original code base, run the code for training, and record the Rouge scores for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "3YlgYMZmt9jG"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length=MAX_LENGTH):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        # Add attention layers\n",
        "        self.attn = nn.Linear(hidden_size * 2, max_length)\n",
        "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        # GRU takes input of combined embedding and attention context\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        # Input shape: [1] (just a single token index)\n",
        "        # Hidden shape: [1, 1, hidden_size]\n",
        "        # Encoder_outputs shape: [max_length, hidden_size]\n",
        "\n",
        "        # 1. Embed the input token\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # [1, 1, hidden_size]\n",
        "        embedded = F.relu(embedded)\n",
        "\n",
        "        # 2. Calculate attention weights\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)  # [1, max_length]\n",
        "\n",
        "        # 3. Apply attention weights to encoder outputs to get context vector\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))  # [1, 1, hidden_size]\n",
        "\n",
        "        # 4. Combine embedded input and attention context vector\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)  # [1, hidden_size*2]\n",
        "        output = self.attn_combine(output).unsqueeze(0)  # [1, 1, hidden_size]\n",
        "        output = F.relu(output)\n",
        "\n",
        "        # 5. Run the GRU for one step\n",
        "        output, hidden = self.gru(output, hidden)  # output: [1, 1, hidden_size]\n",
        "\n",
        "        # 6. Transform GRU output to vocabulary size and apply log softmax\n",
        "        output = self.softmax(self.out(output[0]))  # [1, output_size]\n",
        "\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# Modified evaluate function for attention decoder\n",
        "def evaluate_attention(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, attn_weights = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words\n",
        "\n",
        "# Modified train function for attention decoder\n",
        "def train_attention(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
        "                    decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "# Training function for attention model\n",
        "def trainIters_attention(encoder, decoder, epochs, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    iter = 1\n",
        "    n_iters = len(train_pairs) * epochs\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch: {epoch+1}/{epochs}\")\n",
        "        for training_pair in train_pairs:\n",
        "            training_pair = tensorsFromPair(training_pair)\n",
        "\n",
        "            input_tensor = training_pair[0]\n",
        "            target_tensor = training_pair[1]\n",
        "\n",
        "            loss = train_attention(input_tensor, target_tensor, encoder,\n",
        "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            print_loss_total += loss\n",
        "\n",
        "            if iter % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                            iter, iter / n_iters * 100, print_loss_avg))\n",
        "            iter += 1\n",
        "\n",
        "# Function to evaluate the model randomly\n",
        "def evaluateRandomly_attention(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate_attention(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "# Function to test the model and record ROUGE scores\n",
        "def test_attention(encoder, decoder, testing_pairs):\n",
        "    input = []\n",
        "    gt = []\n",
        "    predict = []\n",
        "    metric_score = {\n",
        "        \"rouge1_fmeasure\":[],\n",
        "        \"rouge1_precision\":[],\n",
        "        \"rouge1_recall\":[],\n",
        "        \"rouge2_fmeasure\":[],\n",
        "        \"rouge2_precision\":[],\n",
        "        \"rouge2_recall\":[]\n",
        "    }\n",
        "    from tqdm import tqdm\n",
        "    for i in tqdm(range(len(testing_pairs))):\n",
        "        pair = testing_pairs[i]\n",
        "        output_words = evaluate_attention(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        input.append(pair[0])\n",
        "        gt.append(pair[1])\n",
        "        predict.append(output_sentence)\n",
        "\n",
        "        try:\n",
        "            rs = rouge(output_sentence, pair[1])\n",
        "        except:\n",
        "            continue\n",
        "        metric_score[\"rouge1_fmeasure\"].append(rs['rouge1_fmeasure'])\n",
        "        metric_score[\"rouge1_precision\"].append(rs['rouge1_precision'])\n",
        "        metric_score[\"rouge1_recall\"].append(rs['rouge1_recall'])\n",
        "        metric_score[\"rouge2_fmeasure\"].append(rs['rouge2_fmeasure'])\n",
        "        metric_score[\"rouge2_precision\"].append(rs['rouge2_precision'])\n",
        "        metric_score[\"rouge2_recall\"].append(rs['rouge2_recall'])\n",
        "\n",
        "    metric_score[\"rouge1_fmeasure\"] = np.array(metric_score[\"rouge1_fmeasure\"]).mean()\n",
        "    metric_score[\"rouge1_precision\"] = np.array(metric_score[\"rouge1_precision\"]).mean()\n",
        "    metric_score[\"rouge1_recall\"] = np.array(metric_score[\"rouge1_recall\"]).mean()\n",
        "    metric_score[\"rouge2_fmeasure\"] = np.array(metric_score[\"rouge2_fmeasure\"]).mean()\n",
        "    metric_score[\"rouge2_precision\"] = np.array(metric_score[\"rouge2_precision\"]).mean()\n",
        "    metric_score[\"rouge2_recall\"] = np.array(metric_score[\"rouge2_recall\"]).mean()\n",
        "\n",
        "    print(\"=== Evaluation score - Rouge score ===\")\n",
        "    print(\"Rouge1 fmeasure:\\t\",metric_score[\"rouge1_fmeasure\"])\n",
        "    print(\"Rouge1 precision:\\t\",metric_score[\"rouge1_precision\"])\n",
        "    print(\"Rouge1 recall:  \\t\",metric_score[\"rouge1_recall\"])\n",
        "    print(\"Rouge2 fmeasure:\\t\",metric_score[\"rouge2_fmeasure\"])\n",
        "    print(\"Rouge2 precision:\\t\",metric_score[\"rouge2_precision\"])\n",
        "    print(\"Rouge2 recall:  \\t\",metric_score[\"rouge2_recall\"])\n",
        "    print(\"=====================================\")\n",
        "    return input,gt,predict,metric_score\n",
        "\n",
        "# Initialize the models\n",
        "hidden_size = 256\n",
        "encoder_attn = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder_attn = AttentionDecoder(hidden_size, output_lang.n_words).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVxgVT6HuGeo"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riDnoVLTuIKU",
        "outputId": "70586763-cce0-423e-e05a-de96bcf19a78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/5\n",
            "1m 56s (- 38m 7s) (5000 4%) 3.2231\n",
            "3m 55s (- 36m 32s) (10000 9%) 2.7234\n",
            "5m 54s (- 34m 42s) (15000 14%) 2.4826\n",
            "7m 54s (- 32m 49s) (20000 19%) 2.3230\n",
            "Epoch: 2/5\n",
            "9m 53s (- 30m 52s) (25000 24%) 2.1728\n",
            "11m 52s (- 28m 56s) (30000 29%) 2.0256\n",
            "13m 54s (- 27m 2s) (35000 33%) 1.9168\n",
            "15m 55s (- 25m 7s) (40000 38%) 1.8526\n",
            "Epoch: 3/5\n",
            "17m 59s (- 23m 12s) (45000 43%) 1.7791\n",
            "20m 0s (- 21m 13s) (50000 48%) 1.6852\n",
            "22m 1s (- 19m 14s) (55000 53%) 1.5918\n",
            "24m 3s (- 17m 16s) (60000 58%) 1.5698\n",
            "Epoch: 4/5\n",
            "26m 5s (- 15m 17s) (65000 63%) 1.5094\n",
            "28m 9s (- 13m 18s) (70000 67%) 1.4695\n",
            "30m 11s (- 11m 18s) (75000 72%) 1.3977\n",
            "32m 13s (- 9m 17s) (80000 77%) 1.4019\n",
            "Epoch: 5/5\n",
            "34m 16s (- 7m 17s) (85000 82%) 1.3575\n",
            "36m 19s (- 5m 16s) (90000 87%) 1.3308\n",
            "38m 20s (- 3m 15s) (95000 92%) 1.3033\n",
            "40m 21s (- 1m 14s) (100000 97%) 1.2814\n"
          ]
        }
      ],
      "source": [
        "trainIters_attention(encoder_attn, decoder_attn, 5, print_every=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFfu0nF-uJ7p"
      },
      "source": [
        "Save the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "R4pRvG3VuNIx"
      },
      "outputs": [],
      "source": [
        "torch.save(encoder_attn.state_dict(), 'encoder_attn.pth')\n",
        "torch.save(decoder_attn.state_dict(), 'decoder_attn.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Sabq_U0fF_R"
      },
      "source": [
        "Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5prbm4nfHsv"
      },
      "outputs": [],
      "source": [
        "# Load the models based on the device type\n",
        "if device.type == 'cuda':\n",
        "    # Load directly to GPU\n",
        "    encoder_attn.load_state_dict(torch.load('encoder_attn.pth'))\n",
        "    decoder_attn.load_state_dict(torch.load('decoder_attn.pth'))\n",
        "else:\n",
        "    # Load to CPU with map_location parameter\n",
        "    encoder_attn.load_state_dict(torch.load('encoder_attn.pth', map_location=device))\n",
        "    decoder_attn.load_state_dict(torch.load('decoder_attn.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNC8PeSwuPF2"
      },
      "source": [
        "Evaluate the models randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3dGpaI2uR8k",
        "outputId": "1ec807d5-396b-4e5e-d5cf-bc1739f419d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> il va leur montrer les documents .\n",
            "= he s going to show them the documents .\n",
            "< he s going to show the the <EOS>\n",
            "\n",
            "> tu es la personne que j attendais .\n",
            "= you re the person i ve been waiting for .\n",
            "< you re the person that i ve been waiting for <EOS>\n",
            "\n",
            "> tu ne vas pas le faire n est ce pas ?\n",
            "= you re not going to do it are you ?\n",
            "< you aren t want to do that are <EOS>\n",
            "\n",
            "> tu as tout a fait raison .\n",
            "= you re exactly right .\n",
            "< you re exactly right . <EOS>\n",
            "\n",
            "> tu as toujours a te plaindre .\n",
            "= you are always complaining .\n",
            "< you re always complaining . . <EOS>\n",
            "\n",
            "> il ne craint personne .\n",
            "= he s not afraid of anyone .\n",
            "< he is not afraid . <EOS>\n",
            "\n",
            "> elles vont te trouver .\n",
            "= they re going to find you .\n",
            "< they re going to find you . <EOS>\n",
            "\n",
            "> je n ai pas peur de la mort .\n",
            "= i m not afraid of death .\n",
            "< i m not afraid of death . <EOS>\n",
            "\n",
            "> je vais repasser un coup de wassingue .\n",
            "= i m going to mop this up .\n",
            "< i m going to watch a book <EOS>\n",
            "\n",
            "> je ne suis pas si mauvais .\n",
            "= i m not that bad .\n",
            "< i m not that bad . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly_attention(encoder_attn, decoder_attn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVSH4EX8uU7n"
      },
      "source": [
        "Test with test pairs and record Rouge scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlLcoM-BuVmm",
        "outputId": "0d3ced31-0083-4a82-bfa8-9419267b6184"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2291/2291 [11:23<00:00,  3.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.59745514\n",
            "Rouge1 precision:\t 0.5673051\n",
            "Rouge1 recall:  \t 0.64152175\n",
            "Rouge2 fmeasure:\t 0.4083379\n",
            "Rouge2 precision:\t 0.38092256\n",
            "Rouge2 recall:  \t 0.45012605\n",
            "=====================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input_attn, gt_attn, predict_attn, score_attn = test_attention(encoder_attn, decoder_attn, test_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVDrhWaOufE0"
      },
      "source": [
        "Task 5: Replace Encoder GRU with Transformer Encoder\n",
        "=======================\n",
        "Change the GRU in Encoder (not Decoder) in the original code base with Transformer Encoder, run the code, and record the Rouge scores for test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "h4I3RMHquhii"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Positional Encoding for Transformer\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Transformer Encoder\n",
        "class EncoderTransformer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, nhead=8, num_layers=2, dropout=0.1):\n",
        "        super(EncoderTransformer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Ensure hidden_size is divisible by nhead\n",
        "        if hidden_size % nhead != 0:\n",
        "            adjusted_nhead = hidden_size // (hidden_size // nhead)\n",
        "            print(f\"Warning: hidden_size ({hidden_size}) not divisible by nhead ({nhead}). Adjusting nhead to {adjusted_nhead}.\")\n",
        "            nhead = adjusted_nhead\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.pos_encoder = PositionalEncoding(hidden_size, dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=hidden_size*4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # Embed input\n",
        "        embedded = self.embedding(input).view(1, 1, -1)  # [1, 1, hidden_size]\n",
        "\n",
        "        # Add positional encoding\n",
        "        embedded = self.pos_encoder(embedded)\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        transformer_output = self.transformer_encoder(embedded)  # [1, 1, hidden_size]\n",
        "\n",
        "        # For GRU compatibility\n",
        "        return transformer_output, transformer_output\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "# Initialize the models\n",
        "hidden_size = 256\n",
        "# For transformer, we need to ensure hidden_size is divisible by nhead\n",
        "nhead = 8  # Number of attention heads\n",
        "num_layers = 2  # Number of transformer encoder layers\n",
        "dropout = 0.1  # Dropout probability\n",
        "\n",
        "encoder_transformer = EncoderTransformer(input_lang.n_words, hidden_size, nhead, num_layers, dropout).to(device)\n",
        "decoder_gru = Decoder(hidden_size, output_lang.n_words).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayb3mW4_ujmq"
      },
      "source": [
        "Train the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNyqKmmvuowU",
        "outputId": "308e9e6c-5310-47b0-f6c3-e8339912ba95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0/5\n",
            "2m 11s (- 43m 2s) (5000 4%) 3.5155\n",
            "4m 23s (- 40m 51s) (10000 9%) 3.2547\n",
            "6m 37s (- 38m 52s) (15000 14%) 3.1818\n",
            "8m 49s (- 36m 41s) (20000 19%) 3.1449\n",
            "Epoch: 1/5\n",
            "11m 2s (- 34m 29s) (25000 24%) 3.0823\n",
            "13m 15s (- 32m 16s) (30000 29%) 3.0238\n",
            "15m 25s (- 30m 1s) (35000 33%) 3.0138\n",
            "17m 35s (- 27m 45s) (40000 38%) 3.0015\n",
            "Epoch: 2/5\n",
            "19m 45s (- 25m 29s) (45000 43%) 2.9659\n",
            "21m 54s (- 23m 15s) (50000 48%) 2.9306\n",
            "24m 4s (- 21m 2s) (55000 53%) 2.9284\n",
            "26m 13s (- 18m 49s) (60000 58%) 2.8976\n",
            "Epoch: 3/5\n",
            "28m 23s (- 16m 37s) (65000 63%) 2.8756\n",
            "30m 31s (- 14m 25s) (70000 67%) 2.8779\n",
            "32m 39s (- 12m 13s) (75000 72%) 2.8427\n",
            "34m 47s (- 10m 2s) (80000 77%) 2.8489\n",
            "Epoch: 4/5\n",
            "37m 0s (- 7m 52s) (85000 82%) 2.8135\n",
            "39m 12s (- 5m 41s) (90000 87%) 2.8050\n",
            "41m 22s (- 3m 31s) (95000 92%) 2.7775\n",
            "43m 31s (- 1m 20s) (100000 97%) 2.7790\n"
          ]
        }
      ],
      "source": [
        "trainIters(encoder_transformer, decoder_gru, 5, print_every=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eZAiT8MupoZ"
      },
      "source": [
        "Save the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "mU0NBXNguq20"
      },
      "outputs": [],
      "source": [
        "torch.save(encoder_transformer.state_dict(), 'encoder_transformer.pth')\n",
        "torch.save(decoder_gru.state_dict(), 'decoder_gru.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVRRhMHGthA9"
      },
      "source": [
        "Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "KdJt9TYztkMx"
      },
      "outputs": [],
      "source": [
        "# Load the models based on the device type\n",
        "if device.type == 'cuda':\n",
        "    # Load directly to GPU\n",
        "    encoder_transformer.load_state_dict(torch.load('encoder_transformer.pth'))\n",
        "    decoder_gru.load_state_dict(torch.load('decoder_gru.pth'))\n",
        "else:\n",
        "    # Load to CPU with map_location parameter\n",
        "    encoder_transformer.load_state_dict(torch.load('encoder_transformer.pth', map_location=device))\n",
        "    decoder_gru.load_state_dict(torch.load('decoder_gru.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxMtVKzWutSs"
      },
      "source": [
        "Evaluate the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRpyhmkQuu6r",
        "outputId": "5cda78a9-ac40-49ab-9449-f02deed7d7aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> je suis enferme hors de ma chambre .\n",
            "= i m locked out of my room .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> je suis heureux que tu l aies apprecie .\n",
            "= i m glad you liked it .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> je suis desolee de vous ennuyer si souvent .\n",
            "= i m sorry to bother you so often .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> je garde cette place pour un ami .\n",
            "= i m saving this seat for a friend .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> il lui a vole un baiser .\n",
            "= he stole a kiss from her .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> il est incapable de mentir .\n",
            "= he is incapable of telling a lie .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> je suis au regime en ce moment .\n",
            "= i m on a diet at the moment .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> vous vous trouvez sur la liste .\n",
            "= you re on the list .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> je retourne a mon bureau .\n",
            "= i m going back to my office .\n",
            "< i m not . . . . <EOS>\n",
            "\n",
            "> elle lui serra la main .\n",
            "= she shook hands with him .\n",
            "< i m not . . . . <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "evaluateRandomly(encoder_transformer, decoder_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GWUYNy2uwI2"
      },
      "source": [
        "Test with test pairs and record Rouge scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7o2-bADu0CB",
        "outputId": "991da1b0-de95-4d6b-e111-f4678b0670e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2291/2291 [18:54<00:00,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluation score - Rouge score ===\n",
            "Rouge1 fmeasure:\t 0.20694938\n",
            "Rouge1 precision:\t 0.26038846\n",
            "Rouge1 recall:  \t 0.17930742\n",
            "Rouge2 fmeasure:\t 0.12091199\n",
            "Rouge2 precision:\t 0.16142878\n",
            "Rouge2 recall:  \t 0.10299744\n",
            "=====================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input_transformer, gt_transformer, predict_transformer, score_transformer = test(encoder_transformer, decoder_gru, test_pairs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3.11.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
